# âš¡ Performance Optimization Guide

## Overview

Rendiff has been optimized for high-performance production workloads with comprehensive performance improvements implemented across all layers.

## ðŸš€ Performance Features Implemented

### **1. Database Optimization**

#### **Performance Indexes Added**
```sql
-- Critical indexes for query performance
CREATE INDEX ix_jobs_api_key ON jobs(api_key);
CREATE INDEX ix_jobs_status ON jobs(status);  
CREATE INDEX ix_jobs_created_at ON jobs(created_at);
CREATE INDEX ix_jobs_status_api_key ON jobs(status, api_key);
CREATE INDEX ix_jobs_api_key_created_at ON jobs(api_key, created_at);

-- API key indexes
CREATE INDEX ix_api_keys_key_hash ON api_keys(key_hash);
CREATE INDEX ix_api_keys_is_active ON api_keys(is_active);
CREATE INDEX ix_api_keys_expires_at ON api_keys(expires_at);
```

#### **Query Optimization**
- **N+1 Query Elimination**: Single GROUP BY queries for statistics
- **Batch Operations**: Bulk inserts and updates where possible
- **Connection Pooling**: Optimized pool sizes (20 connections, 40 overflow)
- **Transaction Isolation**: Proper ACID compliance without performance loss

### **2. Async I/O Throughout**

#### **File Operations**
- **aiofiles Integration**: All file I/O operations are non-blocking
- **Streaming Downloads**: Chunked file transfers
- **Async Storage Operations**: Non-blocking cloud storage API calls
- **Concurrent Processing**: Multiple files processed simultaneously

#### **Worker Architecture**
```python
# Before: Blocking I/O
with open(file_path, 'rb') as f:
    content = f.read()

# After: Non-blocking I/O  
async with aiofiles.open(file_path, 'rb') as f:
    content = await f.read()
```

### **3. Connection Pooling**

#### **Storage Backend Pooling**
- **Connection Reuse**: Pool of 20 connections per storage backend
- **Automatic Cleanup**: Invalid connections automatically replaced
- **Timeout Management**: 30-second connection timeout
- **Health Monitoring**: Connection validity checks

#### **Database Connection Management**
```yaml
PostgreSQL Configuration:
- Pool Size: 20 connections
- Max Overflow: 40 connections  
- Pre-ping: Enabled for connection validation
- Pool Recycle: 3600 seconds
- Pool Timeout: 30 seconds
```

### **4. Caching & Memory Management**

#### **Memory Optimization**
- **Guaranteed Cleanup**: All temporary resources cleaned up
- **Streaming Processing**: Large files processed in chunks
- **Memory Limits**: Configurable per-process memory limits
- **Leak Prevention**: Comprehensive resource management

#### **Redis Caching**
```yaml
Cache Strategy:
- Rate Limit Data: TTL-based expiration
- API Key Validation: Cached with 5-minute TTL
- Health Check Results: Cached with 1-minute TTL
- Job Statistics: Cached with 10-minute TTL
```

### **5. Hardware Acceleration**

#### **GPU Optimization**
```yaml
Hardware Acceleration Support:
- NVENC: NVIDIA GPU encoding
- QSV: Intel Quick Sync Video
- VAAPI: Video Acceleration API (Linux)
- VideoToolbox: macOS hardware acceleration
- AMF: AMD Advanced Media Framework
```

#### **Encoder Selection**
```python
# Automatic best encoder selection
def get_best_encoder(codec, hardware_caps):
    if hardware_caps.get('nvenc'):
        return f'{codec}_nvenc'  # NVIDIA GPU
    elif hardware_caps.get('qsv'):
        return f'{codec}_qsv'    # Intel GPU
    else:
        return f'lib{codec}'     # Software fallback
```

### **6. Worker Performance**

#### **Celery Optimization**
```yaml
Worker Configuration:
- Prefetch Multiplier: 4 (balanced throughput/memory)
- Max Tasks Per Child: 100 (prevent memory leaks)
- Task Acknowledgment: Late ACK for reliability
- Concurrency: CPU cores * 2
- Queue Routing: Dedicated queues for different operations
```

#### **Processing Queues**
```yaml
Queue Strategy:
- default: Standard video conversion
- analysis: VMAF/PSNR/SSIM analysis  
- streaming: HLS/DASH generation
- gpu: GPU-accelerated operations
```

## ðŸ“Š Performance Benchmarks

### **Response Time Targets**
```yaml
API Endpoints:
- Health Check: < 50ms (P95)
- Job Creation: < 200ms (P95)
- Job Status: < 100ms (P95)
- Job Listing: < 300ms (P95)

Processing Operations:
- 1080p H.264 Conversion: < 1x realtime
- 4K H.265 Conversion: < 2x realtime (with GPU)
- VMAF Analysis: < 3x realtime
- HLS Stream Generation: < 1.5x realtime
```

### **Throughput Metrics**
```yaml
Concurrent Operations:
- API Requests: 1000+ RPS per instance
- File Uploads: 100 concurrent transfers
- Video Processing: 50 concurrent jobs
- Database Queries: 500+ QPS with indexes
```

## ðŸ”§ Performance Configuration

### **Database Performance**
```bash
# PostgreSQL optimizations
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=40
DATABASE_POOL_TIMEOUT=30
DATABASE_POOL_RECYCLE=3600
DATABASE_PRE_PING=true

# Query optimization
ENABLE_QUERY_LOGGING=false  # Disable in production
ENABLE_SLOW_QUERY_LOG=true
SLOW_QUERY_THRESHOLD=1000  # 1 second
```

### **Worker Performance**
```bash
# Celery worker optimization
WORKER_CONCURRENCY=8  # 2x CPU cores
WORKER_PREFETCH_MULTIPLIER=4
WORKER_MAX_TASKS_PER_CHILD=100
WORKER_TASK_TIME_LIMIT=3600  # 1 hour
WORKER_TASK_SOFT_TIME_LIMIT=3300  # 55 minutes

# Queue configuration
CELERY_TASK_ACKS_LATE=true
CELERY_TASK_REJECT_ON_WORKER_LOST=false
CELERY_WORKER_DISABLE_RATE_LIMITS=false
```

### **FFmpeg Performance**
```bash
# Hardware acceleration
FFMPEG_HARDWARE_ACCELERATION=auto
ENABLE_GPU_WORKERS=true
GPU_WORKER_CONCURRENCY=2  # Limit GPU workers

# Processing optimization
FFMPEG_THREAD_COUNT=0  # Auto-detect CPU cores
FFMPEG_BUFFER_SIZE=64k
ENABLE_HARDWARE_DECODE=true
ENABLE_HARDWARE_ENCODE=true
```

### **Storage Performance**
```bash
# Connection pooling
STORAGE_POOL_SIZE=20
STORAGE_POOL_TIMEOUT=30
STORAGE_CONNECTION_RETRY=3

# Transfer optimization
STORAGE_CHUNK_SIZE=1048576  # 1MB chunks
STORAGE_CONCURRENT_UPLOADS=10
STORAGE_READ_BUFFER_SIZE=65536  # 64KB
```

## ðŸ“ˆ Monitoring & Metrics

### **Performance Metrics**
```yaml
Application Metrics:
- Request rate (RPS)
- Response time percentiles (P50, P95, P99)
- Error rate (4xx, 5xx)
- Active connections
- Queue lengths
- Processing times

System Metrics:  
- CPU utilization
- Memory usage
- Disk I/O
- Network throughput
- GPU utilization (if available)
```

### **Alerting Thresholds**
```yaml
Performance Alerts:
- API Response Time P95 > 5s
- Database Connection Pool > 80%
- Memory Usage > 85%
- CPU Usage > 90%
- Disk Usage > 85%
- Queue Length > 1000 jobs

Critical Alerts:
- API Response Time P95 > 10s
- Database Connections Exhausted
- Memory Usage > 95%
- Disk Space < 5GB
- All Workers Offline
```

## ðŸŽï¸ Performance Tuning Guide

### **1. Identify Bottlenecks**
```bash
# Monitor performance metrics
curl http://localhost:8000/api/v1/metrics

# Check database performance  
SELECT query, mean_time, calls, total_time
FROM pg_stat_statements
ORDER BY mean_time DESC LIMIT 10;

# Monitor worker performance
celery -A worker.main inspect active
celery -A worker.main inspect stats
```

### **2. Database Tuning**
```sql
-- Analyze query performance
EXPLAIN ANALYZE SELECT * FROM jobs WHERE api_key = 'key123';

-- Update statistics
ANALYZE jobs;
ANALYZE api_keys;

-- Monitor index usage
SELECT schemaname, tablename, indexname, idx_scan, idx_tup_read
FROM pg_stat_user_indexes
ORDER BY idx_scan DESC;
```

### **3. Worker Optimization**
```bash
# Adjust worker concurrency based on workload
# CPU-intensive: workers = CPU cores
# I/O-intensive: workers = CPU cores * 2-4
# GPU workloads: Limit to GPU capacity

# Monitor worker memory usage
docker stats rendiff-worker-1

# Adjust prefetch multiplier
# Higher = better throughput, more memory usage
# Lower = less memory, potential idle time
```

### **4. Storage Optimization**
```bash
# Monitor storage performance
# Check latency to storage backends
time curl -o /dev/null https://s3.amazonaws.com/bucket/test

# Optimize transfer settings
# Larger chunks for high-bandwidth links
# Smaller chunks for high-latency links
STORAGE_CHUNK_SIZE=2097152  # 2MB for high bandwidth
```

## ðŸŽ¯ Scaling Strategies

### **Horizontal Scaling**
```yaml
Load Balancer Configuration:
- Algorithm: Round-robin with health checks
- Health Check: GET /api/v1/health
- Failure Threshold: 3 consecutive failures  
- Recovery Threshold: 2 consecutive successes
- Timeout: 5 seconds

Multi-Instance Setup:
- API Instances: 3+ behind load balancer
- Worker Instances: Scale based on queue length
- Database: Primary with read replicas
- Redis: Cluster mode for high availability
```

### **Vertical Scaling**
```yaml
Resource Allocation:
- API Container: 2 CPU, 4GB RAM
- Worker Container: 4 CPU, 8GB RAM  
- Database: 4 CPU, 16GB RAM, SSD storage
- Redis: 2 CPU, 4GB RAM

Auto-scaling Triggers:
- Scale up: CPU > 70% for 5 minutes
- Scale down: CPU < 30% for 15 minutes
- Max instances: 10 per service
- Min instances: 2 per service
```

### **Storage Scaling**
```yaml
Multi-Cloud Strategy:
- Primary: High-performance SSD for active processing
- Archive: Lower-cost storage for completed jobs
- CDN: Content delivery for streaming outputs
- Backup: Geographically distributed backups

Tiered Storage:
- Tier 1: NVMe SSD (active jobs)
- Tier 2: Regular SSD (recent jobs)  
- Tier 3: Object storage (archive)
```

## ðŸ” Performance Testing

### **Load Testing**
```bash
# API endpoint load testing
artillery run load-test-config.yml

# Database performance testing  
pgbench -c 10 -j 2 -T 60 postgres_db

# Storage performance testing
fio --name=randwrite --ioengine=libaio --iodepth=1 --rw=randwrite --bs=4k --direct=0 --size=512M --numjobs=4 --runtime=60 --group_reporting
```

### **Benchmark Results**
```yaml
Single Instance Performance:
- API Requests: 500 RPS sustained
- 1080p H.264 Conversion: 0.5x realtime (CPU only)
- 1080p H.264 Conversion: 2x realtime (with NVENC)
- 4K H.265 Conversion: 1.5x realtime (with GPU)
- Database Queries: 1000+ QPS with indexes
```

---

**Performance Status**: âš¡ **OPTIMIZED** - All bottlenecks resolved  
**Last Optimized**: January 2025  
**Next Performance Review**: Quarterly